{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom transformers import LlamaTokenizer, LlamaModel\n\n# 加载预训练的LLaMA 2模型和tokenizer\ntokenizer = LlamaTokenizer.from_pretrained('LLaMA-2')\nmodel = LlamaModel.from_pretrained('LLaMA-2')\n\n# 超参数\nbatch_size = 32\nlearning_rate = 1e-4\nnum_epochs = 10\ntemperature = 0.07\n\n# 构建笔记压缩prompt\ndef build_prompt(note):\n    instruction = \"Extract the note information in json format, compress it into one word for recommendation.\"\n    input_note = f\"{{'title': '{note['title']}', 'content': '{note['content']}'}}\"\n    prompt = f\"[BOS]{instruction} {input_note} The compression word is: '[EMB]'.[EOS]\"\n    return prompt\n\n# 生成式对比学习任务\nclass GenerativeContrastiveLearning(nn.Module):\n    def __init__(self, model):\n        super(GenerativeContrastiveLearning, self).__init__()\n        self.model = model\n        self.fc = nn.Linear(model.config.hidden_size, model.config.hidden_size)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state\n        emb_token_idx = (input_ids == tokenizer.convert_tokens_to_ids('[EMB]')).nonzero(as_tuple=True)\n        emb_vectors = hidden_states[emb_token_idx]\n        emb_vectors = self.fc(emb_vectors)\n        return emb_vectors\n\n# 协同监督微调任务\nclass CollaborativeSupervisedFineTuning(nn.Module):\n    def __init__(self, model):\n        super(CollaborativeSupervisedFineTuning, self).__init__()\n        self.model = model\n        self.classifier = nn.Linear(model.config.hidden_size, num_labels)  # num_labels为类别数量\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids, attention_mask=attention_mask)\n        hidden_states = outputs.last_hidden_state\n        cls_token_idx = (input_ids == tokenizer.cls_token_id).nonzero(as_tuple=True)\n        cls_vectors = hidden_states[cls_token_idx]\n        logits = self.classifier(cls_vectors)\n        return logits\n\n# 损失函数\ndef contrastive_loss(embeddings, positive_pairs, negative_pairs, temperature):\n    pos_sim = torch.cosine_similarity(embeddings[positive_pairs[:, 0]], embeddings[positive_pairs[:, 1]])\n    neg_sim = torch.cosine_similarity(embeddings[negative_pairs[:, 0]], embeddings[negative_pairs[:, 1]])\n    loss = -torch.log(torch.exp(pos_sim / temperature) / (torch.exp(pos_sim / temperature) + torch.exp(neg_sim / temperature)))\n    return loss.mean()\n\n# 数据加载和预处理\ndef preprocess_data(notes):\n    input_ids, attention_masks = [], []\n    for note in notes:\n        prompt = build_prompt(note)\n        encoded_prompt = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True)\n        input_ids.append(encoded_prompt['input_ids'])\n        attention_masks.append(encoded_prompt['attention_mask'])\n    return torch.cat(input_ids), torch.cat(attention_masks)\n\n# 加载数据\nnotes = [\n    {'title': 'Note 1', 'content': 'Content of note 1'},\n    {'title': 'Note 2', 'content': 'Content of note 2'},\n    # 添加更多笔记数据\n]\n\ninput_ids, attention_masks = preprocess_data(notes)\n\n# 模型初始化\ngcl_model = GenerativeContrastiveLearning(model)\ncsft_model = CollaborativeSupervisedFineTuning(model)\n\n# 优化器\noptimizer = optim.Adam(list(gcl_model.parameters()) + list(csft_model.parameters()), lr=learning_rate)\n\n# 训练循环\nfor epoch in range(num_epochs):\n    gcl_model.train()\n    csft_model.train()\n    \n    # 前向传播\n    embeddings = gcl_model(input_ids, attention_mask=attention_masks)\n    logits = csft_model(input_ids, attention_mask=attention_masks)\n    \n    # 计算损失\n    gcl_loss = contrastive_loss(embeddings, positive_pairs, negative_pairs, temperature)\n    csft_loss = nn.CrossEntropyLoss()(logits, labels)  # labels为真实标签\n    loss = gcl_loss + csft_loss\n    \n    # 反向传播和优化\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n\n# 保存模型\ntorch.save(gcl_model.state_dict(), 'gcl_model.pth')\ntorch.save(csft_model.state_dict(), 'csft_model.pth')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}